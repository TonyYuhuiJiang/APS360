{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"APS360 Project.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"8R_AJPfF-rm8","colab_type":"text"},"source":["# Data Loading and Splitting"]},{"cell_type":"code","metadata":{"id":"0stwkHiR-u2Y","colab_type":"code","outputId":"3884db50-8ad4-4507-8c58-dba592643ea3","executionInfo":{"status":"ok","timestamp":1585786043466,"user_tz":420,"elapsed":23736,"user":{"displayName":"Liu Steven","photoUrl":"","userId":"09537302183948925847"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KaeQdLB3OX5e","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import numpy as np\n","import time\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3hEhnT-wOd0o","colab_type":"code","colab":{}},"source":["# Loading Gesture Images from Google Drive\n","\n","# location on Google Drive\n","orig_path = '/content/gdrive/My Drive/APS360/APS360 Final Project/Dataset/dataset2/images/TRAIN'\n","test_path = '/content/gdrive/My Drive/APS360/APS360 Final Project/Dataset/dataset2/images/TEST'\n","\n","# Transform Settings - Do not use RandomResizedCrop\n","transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n","\n","orig_set = torchvision.datasets.ImageFolder(orig_path, transform=transform)\n","\n","# val_set = torch.utils.data.Subset(orig_set, range(a))  \n","# train_set = torch.utils.data.Subset(orig_set, range(a, b))   \n","n = len(orig_set)\n","n_val = int(0.25*n)\n","val_set, train_set = torch.utils.data.random_split(orig_set, [n_val, n-n_val])\n","\n","test_set = torchvision.datasets.ImageFolder(test_path, transform=transform)\n","\n","print(\"The length of train set: {}\".format(len(train_set)))\n","print(\"The length of validation set: {}\".format(len(val_set)))\n","print(\"The length of test set: {}\".format(len(test_set)))\n","\n","\n","# Prepare Dataloader\n","batch_size = 32\n","num_workers = 1\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, \n","                                           num_workers=num_workers, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, \n","                                           num_workers=num_workers, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, \n","                                           num_workers=num_workers, shuffle=True)\n","\n","\n","# Verification Step - obtain one batch of images\n","dataiter = iter(val_loader)\n","images, labels = dataiter.next()\n","images = images.numpy() # convert images to numpy for display\n","\n","classes = ['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL']\n","\n","# plot the images in the batch, along with the corresponding labels\n","fig = plt.figure(figsize=(25, 4))\n","for idx in np.arange(20):\n","    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n","    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n","    ax.set_title(classes[labels[idx]])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wfsk1GLtXI3W","colab_type":"text"},"source":["# Data Processing: Finding Bounding Box"]},{"cell_type":"code","metadata":{"id":"MwTSsSMWXQDQ","colab_type":"code","colab":{}},"source":["# import the original images and labels\n","\n","import cv2\n","import csv\n","import re\n","from PIL import Image\n","import os, os.path\n","from matplotlib.image import imread\n","\n","\n","# get images, labels from original data\n","orig_dataset_path = '/content/gdrive/My Drive/APS360/Dataset/dataset1/'\n","img_path = orig_dataset_path + 'JPEGImages/'\n","label_path = orig_dataset_path + 'labels.csv'\n","\n","\n","# with open(label_path) as f:\n","#     reader = csv.reader(f)\n","#     data = [tuple(row) for row in reader]\n","#     data = data[1:]\n","\n","# # create label dictionary for each image\n","# # label[img_index] = label for image with id img_index\n","# all_labels = {}\n","# for i in range(len(data)):\n","#     all_labels[int(data[i][1])] = data[i][2].split(',')\n","\n","# for i in range(len(data)):\n","#     if(len(all_labels[i]) > 1):\n","#         for j in range(len(all_labels[i])):\n","#             all_labels[i][j] = all_labels[i][j].strip()\n","\n","\n","# imgs[img_index] = img with id img_index\n","labels = {}\n","imgs = {}\n","for f in os.listdir(img_path):\n","    index = int(re.findall('\\d+', f)[0])\n","    imgs[index] =  imread(img_path + f)\n","    # labels[index] = all_labels[index]\n","\n","print(len(imgs))\n","print(len(labels))\n","\n","# i = 0\n","# print(labels[i])\n","# plt.imshow(imgs[i])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YpEDXPyGXTdp","colab_type":"code","colab":{}},"source":["# functions to find wbc bounding boxes in an img\n","\n","def distance(x1, y1, x2, y2):\n","    return ((x2-x1)**2 + (y2-y1)**2) ** 0.5\n","\n","def find_min_distance(rect1,rect2):\n","  dist = []\n","  \n","  dist.append(distance(rect1[0],rect1[1],rect2[0]+rect2[2]/2,rect2[1]+rect2[3]/2))\n","  dist.append(distance(rect1[0]+rect1[2],rect1[1],rect2[0]+rect2[2]/2,rect2[1]+rect2[3]/2))\n","  dist.append(distance(rect1[0],rect1[1]+rect1[3],rect2[0]+rect2[2]/2,rect2[1]+rect2[3]/2))\n","  dist.append(distance(rect1[0]+rect1[2],rect1[1]+rect1[3],rect2[0]+rect2[2]/2,rect2[1]+rect2[3]/2))\n","  return min(dist)\n","\n","\n","def merge_by_iou(rects):\n","    clusters = []\n","    merge = 1\n","    used_rect_list = []\n","\n","    for i in range(len(rects)):\n","        if not i in used_rect_list:\n","            temp = []\n","            temp.append(rects[i])\n","            used_rect_list.append(i)\n","            for j in range(i+1,len(rects)):\n","                if not j in used_rect_list:\n","                    iou = iou_val(\n","                        rects[i][0], rects[i][0] + rects[i][2], rects[i][1], rects[i][1] + rects[i][3], \n","                        rects[j][0], rects[j][0] + rects[j][2], rects[j][1], rects[j][1] + rects[j][3])\n","                    if iou > 0.9:\n","                        temp.append(rects[j])\n","                        used_rect_list.append(j)\n","            clusters.append(temp)\n","\n","                \n","    output = []\n","    for i in range(len(clusters)):\n","      lx, ly, rx, ry = 10000, 10000, -10000, -10000\n","      for j in range(len(clusters[i])):\n","        if lx > clusters[i][j][0]:\n","          lx = clusters[i][j][0]\n","        if ly > clusters[i][j][1]:\n","          ly = clusters[i][j][1]\n","        if rx < clusters[i][j][0]+clusters[i][j][2]:\n","          rx = clusters[i][j][0]+clusters[i][j][2]\n","        if ry < clusters[i][j][1]+clusters[i][j][3]:\n","          ry = clusters[i][j][1]+clusters[i][j][3]\n","      output.append((lx,ly,rx-lx,ry-ly))\n","\n","    return output\n","            \n","\n","# given: a list of initial bboxs\n","# return: a list of merged bounding boxes\n","# \n","# need to find 2d point clustering and merge the clustering bboxs\n","# to merge, find xmin, xmax, ymin, ymax of all merging bbox and add margin\n","def merge_bbox(rects):\n","    # for i in range(len(rects)):\n","    #   print(rects[i])\n","    #first dismiss the rect that is too small\n","    want = []\n","    for rect in rects:\n","      if rect[3] > 0 or rect[2]>0:\n","        want.append(rect)\n","    #now seperate them into different clusters, if the rects are within 50 pixels assume they are same cluster\n","    clusters = []\n","    if len(want) == 1: \n","      clusters.append([want[0]])\n","\n","\n","    used_rect_list = []\n","    for i in range(len(want)):\n","      # print(used_rect_list)\n","      if not i in used_rect_list:\n","        temp = []\n","        temp.append(want[i])\n","        used_rect_list.append(i)\n","        for j in range(i+1,len(want)):\n","          if not j in used_rect_list:\n","            min_dist = find_min_distance(want[i],want[j])\n","\n","            ## The distance threshold\n","            if min_dist < (want[i][2]+want[i][3])/2.5:\n","              temp.append(want[j])\n","              used_rect_list.append(j)\n","        clusters.append(temp)\n","    #return boxes\n","    output = []\n","    for i in range(len(clusters)):\n","      lx, ly, rx, ry, w, h = 10000, 10000, -10000, -10000, 0, 0\n","      for j in range(len(clusters[i])):\n","        if lx > clusters[i][j][0]:\n","          lx = clusters[i][j][0]\n","        if ly > clusters[i][j][1]:\n","          ly = clusters[i][j][1]\n","        if rx < clusters[i][j][0]+clusters[i][j][2]:\n","          rx = clusters[i][j][0]+clusters[i][j][2]\n","        if ry < clusters[i][j][1]+clusters[i][j][3]:\n","          ry = clusters[i][j][1]+clusters[i][j][3]\n","      output.append((lx,ly,rx-lx,ry-ly))\n","    # print(len(output))\n","\n","    real_output = []\n","    for a in range(len(output)):\n","      if output[a][2]*output[a][3] > 1000:\n","\n","        x_min = max(output[a][0] - 12, 0)\n","        y_min = max(output[a][1] - 12, 0)\n","        x_max = min(output[a][0] + output[a][2] + 24, 640)\n","        y_max = min(output[a][1] + output[a][3] + 24, 480)\n","        width = x_max - x_min\n","        height = y_max - y_min\n","\n","        real_output.append((x_min,y_min,width,height))\n","\n","    return real_output\n","\n","def find_wbc_bbox(input_img):\n","    \n","    # min area for bbox bounded area\n","    limit_area = 10\n","    x = 0   \n","    y = 0   \n","    w = 0   \n","    h = 0   \n","\n","    img1 = input_img.copy()\n","    img = cv2.add(img1, 0.70)\n","    img_3 = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n","    mask1 = cv2.inRange(img_3, (90,140,0), (255,255,255))   \n","    mask2 = cv2.inRange(img_3, (90,90,0), (255,255,255))   \n","    mask1 = cv2.equalizeHist(mask1)\n","    mask2 = cv2.equalizeHist(mask2)\n","    \n","    mask = mask1 + mask2   \n","    kernel = np.ones((1,4),np.uint8)   \n","    mask = cv2.dilate(mask,kernel,iterations = 1)   \n","    kernel_close = np.ones((3,3),np.uint8)\n","    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel_close)   \n","    blur2 = cv2.medianBlur(mask,7)   \n","    canny = cv2.Canny(blur2, 100,200)   \n","    contours, hierarchy = cv2.findContours(canny,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE) \n","\n","    rects = []\n","    fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(5, 5))\n","\n","    for cnt in contours:   \n","        if cv2.contourArea(cnt) >= limit_area:\n","            x, y, w, h = cv2.boundingRect(cnt)\n","\n","            rects.append((x, y, w, h))\n","\n","            roi = blur2[y:y+h, x:x+w]\n","            image_roi = cv2.resize(roi, (128,128), interpolation=cv2.INTER_AREA)\n","            image_roi = cv2.medianBlur(image_roi, 5)\n","            (T, thresh) = cv2.threshold(image_roi, 10, 255, cv2.THRESH_BINARY)\n","            contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","            contours = [i for i in contours if cv2.contourArea(i) <= 5000]\n","            cv2.fillPoly(thresh, contours, color=(0,0,0))\n","            for bbox in rects:\n","                cv2.rectangle(img1, (bbox[0], bbox[1]), (bbox[0] + bbox[2], bbox[1] + bbox[3]), (255,0,0), 3)\n","    \n","    ax.imshow(img1)\n","    # find merged bboxs\n","    merged_bbox = merge_bbox(rects)\n","\n","    bbox_count = len(merged_bbox)\n","    for i in range(bbox_count):\n","        merged_bbox = merge_by_iou(merged_bbox)\n","\n","\n","\n","    return merged_bbox"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9iq8RDDXU8B","colab_type":"code","colab":{}},"source":["# show original img\n","fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(5, 5))\n","ax.imshow(imgs[4])\n","\n","merged_bbox = find_wbc_bbox(imgs[4])\n","imgg = imgs[4].copy()\n","fig1, ax1 = plt.subplots(ncols=1, nrows=1, figsize=(5, 5))\n","for bbox in merged_bbox:\n","        cv2.rectangle(imgg, (bbox[0], bbox[1]), (bbox[0] + bbox[2], bbox[1] + bbox[3]), (255,0,0), 3)\n","ax1.imshow(imgg)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zo7OC1IsXXM9","colab_type":"code","colab":{}},"source":["for i in sorted(dataset.keys()): \n","    if(i in range(200, 410)):\n","        img = dataset[i][0].copy()\n","        label_list = dataset[i][1]\n","        fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(3, 3))\n","        j = str(i)+ ' ' + label_list[0][0]\n","        plt.title(j)\n","\n","        for j in label_list:\n","            x, y, w, h = j[1]\n","            cv2.rectangle(img, (x, y), (x+w,y+h), (255,0,0), 3)\n","\n","        ax.imshow(img)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zuw3RH2hXbnV","colab_type":"code","colab":{}},"source":["label_list = []\n","for i in sorted(dataset.keys()):\n","    label_list_i = dataset[i][1]\n","    label_list.append((i, label_list_i))\n","\n","print(label_list[0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q0XCpZ91BCf-","colab_type":"text"},"source":["# Data Processing: Merge Images and Merge Labels"]},{"cell_type":"code","metadata":{"id":"eHwgt-aOBS19","colab_type":"code","colab":{}},"source":["def merge_images(image1, image2, image3, image4):\n","    \"\"\"Merge two images into one, displayed side by side\n","    :param file1: path to first image file\n","    :param file2: path to second image file\n","    :return: the merged Image object\n","    \"\"\"\n","    # image1 = Image.open(file1)\n","    # image2 = Image.open(file2)\n","    # image3 = Image.open(file3)\n","    # image4 = Image.open(file4)\n","    (width, height) = image1.size\n","\n","    result_width = 2*width\n","    result_height = 2*height\n","\n","    result = Image.new('RGB', (result_width, result_height))\n","    result.paste(im=image1, box=(0, 0))\n","    result.paste(im=image2, box=(width, 0))\n","    result.paste(im=image3, box=(0, height))\n","    result.paste(im=image4, box=(width, height))\n","    result = result.resize((224, 224))\n","    return result\n","\n","def merge_labels(l1, l2, l3, l4):\n","  merged_label = []\n","  for i in l1:\n","    cls = i[0]\n","\n","    label = list(i[1])\n","    label = [x/2 for x in label]\n","    w_ratio = 224/640\n","    h_ratio = 224/480\n","    label[0] = label[0]*w_ratio\n","    label[1] = label[1]*h_ratio\n","    label[2] = label[2]*w_ratio\n","    label[3] = label[3]*h_ratio\n","    label = [round(x) for x in label]\n","    label = tuple(label)\n","    merged_label.append((cls,label))\n","  for i in l2:\n","    cls = i[0]\n","    label = list(i[1])\n","    label = [x/2 for x in label]\n","    label[0] += 640/2\n","    w_ratio = 224/640\n","    h_ratio = 224/480\n","    label[0] = label[0]*w_ratio\n","    label[1] = label[1]*h_ratio\n","    label[2] = label[2]*w_ratio\n","    label[3] = label[3]*h_ratio\n","    label = [round(x) for x in label]\n","    label = tuple(label)\n","    merged_label.append((cls,label))\n","  for i in l3:\n","    cls = i[0]\n","    label = list(i[1])\n","    label = [x/2 for x in label]\n","    label[1] += 480/2\n","    w_ratio = 224/640\n","    h_ratio = 224/480\n","    label[0] = label[0]*w_ratio\n","    label[1] = label[1]*h_ratio\n","    label[2] = label[2]*w_ratio\n","    label[3] = label[3]*h_ratio\n","    label = [round(x) for x in label]\n","    label = tuple(label)\n","    merged_label.append((cls,label))\n","  for i in l4:\n","    cls = i[0]\n","    label = list(i[1])\n","    label = [x/2 for x in label]\n","    label[0] += 640/2\n","    label[1] += 480/2\n","    w_ratio = 224/640\n","    h_ratio = 224/480\n","    label[0] = label[0]*w_ratio\n","    label[1] = label[1]*h_ratio\n","    label[2] = label[2]*w_ratio\n","    label[3] = label[3]*h_ratio\n","    label = [round(x) for x in label]\n","    label = tuple(label)\n","    merged_label.append((cls,label))\n","  return merged_label"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mIqWoRlCBa5w","colab_type":"code","colab":{}},"source":["def generate_rand_image(src_path, start, end):\n","  while True:\n","    num = random.randint(start, end)\n","    s = '%0*d' % (3, num)\n","    image_path = src_path + \"/BloodImage_00\" + s+ \".jpg\"\n","    try:\n","      image = Image.open(image_path)\n","      return image, num\n","    except FileNotFoundError:\n","      # print(\"Can't find this path: {}, generate a new one\")\n","      continue\n","  print(\"we shouldn't get here\")\n","  return None\n","\n","\n","def get_bbox_list(idx):\n","  csv_path = '/content/gdrive/My Drive/APS360/APS360 Final Project/Dataset/dataset1/bbox2.csv'\n","  with open(csv_path) as f:\n","    f_reader = csv.reader(f, delimiter=',')\n","    for row in f_reader:\n","      if row[0] == str(idx):\n","        ret = list(ast.literal_eval(row[1]))\n","        return ret\n","    else:\n","      print(\"so bad, idx is {}\".format(idx))\n","\n","def get_merged_list(idx):\n","  csv_path = '/content/gdrive/My Drive/APS360/APS360 Final Project/Dataset/dataset1/Merged Labels/ValidationLabels.csv'\n","  with open(csv_path) as f:\n","    f_reader = csv.reader(f, delimiter=',')\n","    for row in f_reader:\n","      if row[0] == str(idx):\n","        ret = list(ast.literal_eval(row[1]))\n","        return ret\n","    else:\n","      print(\"so bad, idx is {}\".format(idx))\n","\n","def build_merged_dataset(s):\n","  src_path = '/content/gdrive/My Drive/APS360/APS360 Final Project/Dataset/dataset1/JPEGImages'\n","  dest_path = '/content/gdrive/My Drive/APS360/APS360 Final Project/Dataset/dataset1/Merged Images2/'+s\n","\n","  #add this to create list of info\n","  info_list = []\n","  start = 0\n","  end = 0\n","  #count for each class\n","  if s==\"Train\":\n","    num = 1000\n","    start = 0\n","    end = 200\n","  elif s==\"Validation\":\n","    num = 500\n","    start = 201\n","    end = 301\n","  else:\n","    num = 500\n","    start = 302\n","    end = 410\n","  for i in range(0, num):\n","    image1,idx1 = generate_rand_image(src_path, start, end)\n","    image2,idx2 = generate_rand_image(src_path, start, end)\n","    image3,idx3 = generate_rand_image(src_path, start, end)\n","    image4,idx4 = generate_rand_image(src_path, start, end)\n","    l1 = get_bbox_list(idx1)\n","    l2 = get_bbox_list(idx2)\n","    l3 = get_bbox_list(idx3)\n","    l4 = get_bbox_list(idx4)\n","    merged_image = merge_images(image1, image2, image3, image4)\n","    merged_label = merge_labels(l1, l2, l3, l4)\n","    # plot_img_with_bbx(merged_image, merged_label)\n","\n","    #append merged image to info list\n","    merged_label.sort(key=lambda a: a[0])\n","    info_list.append(merged_label)\n","\n","    merged_path = dest_path+\"/merged_\"+str(i)+\".jpg\"\n","    merged_image.save(merged_path)\n","  return info_list\n","\n","def plot_img_with_bbx(img, label_list):\n","  img_copy = np.array(img.copy())\n","  fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(3,3))\n","  for i in label_list:\n","    x,y,w,h = i[1]\n","    cv2.rectangle(img_copy,(x,y),(x+w,y+h),(255,0,0),3)\n","  ax.imshow(img_copy)\n","\n","def count_class(info_list):\n","    class_table = {} #creates a dictionary\n","\n","    for items in info_list:\n","        if items[0] not in class_table:\n","            class_table[items[0]] = 1\n","        else:\n","            class_table[items[0]] +=1\n","\n","    return class_table\n","\n","def create_label_csv(info_list, s):\n","    path = '/content/gdrive/My Drive/APS360/APS360 Final Project/Dataset/dataset1/Merged Labels2/'\n","    name = s+'Labels.csv'\n","    with open(path + name, 'w', newline='') as file:\n","        field_names = ['index', 'class_info', 'class_count']\n","        writer = csv.DictWriter(file, fieldnames=field_names)\n","        writer.writeheader()\n","        index_num = 0\n","        for items in info_list:\n","            writer.writerow({'index': index_num, 'class_info': items, 'class_count': count_class(items)})\n","            index_num+=1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JTO-AE8aBliR","colab_type":"text"},"source":["# Data Processing: Build Train, Validation and Test Dataset for R-CNN Model"]},{"cell_type":"code","metadata":{"id":"YbHLEXY3B3sA","colab_type":"code","colab":{}},"source":["train_info_list = build_merged_dataset(\"Train\") create_label_csv(train_info_list, \"Train\")\n","\n","val_info_list = build_merged_dataset(\"Validation\") create_label_csv(val_info_list, \"Validation\")\n","\n","test_info_list = build_merged_dataset(\"Test\") create_label_csv(test_info_list, \"Test\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AIzhodQUGvOR","colab_type":"text"},"source":["# Baseline Model - KNN"]},{"cell_type":"code","metadata":{"id":"IdAzn95mG1Uq","colab_type":"code","colab":{}},"source":["!pip install imutils"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gprm5Ham9LTT","colab_type":"code","colab":{}},"source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split\n","from imutils import paths\n","import numpy as np\n","import argparse\n","import imutils\n","import cv2\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ARzVUD6G9NCT","colab_type":"code","colab":{}},"source":["def image_to_feature_vector(image, size=(32, 32)):\n","\t# resize the image to a fixed size, then flatten the image into\n","\t# a list of raw pixel intensities\n","\treturn cv2.resize(image, size).flatten()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MTqlrsB_9PLI","colab_type":"code","colab":{}},"source":["def extract_color_histogram(image, bins=(8, 8, 8)):\n","\t# extract a 3D color histogram from the HSV color space using\n","\t# the supplied number of `bins` per channel\n","\thsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","\thist = cv2.calcHist([hsv], [0, 1, 2], None, bins,\n","\t\t[0, 180, 0, 256, 0, 256])\n","\t# handle normalizing the histogram if we are using OpenCV 2.4.X\n","\tif imutils.is_cv2():\n","\t\thist = cv2.normalize(hist)\n","\t# otherwise, perform \"in place\" normalization in OpenCV 3 (I\n","\t# personally hate the way this is done\n","\telse:\n","\t\tcv2.normalize(hist, hist)\n","\t# return the flattened histogram as the feature vector\n","\treturn hist.flatten()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nnn5BBws9PzU","colab_type":"code","colab":{}},"source":["# loop over the input images\n","for (i, imagePath) in enumerate(imagePaths):\n","  \n","\timage = cv2.imread(imagePath)\n","\tlabel = imagePath.split(os.path.sep)[-1].split(\".\")[0]\n","\t# extract raw pixel intensity \"features\", followed by a color\n","\t# histogram to characterize the color distribution of the pixels\n","\t# in the image\n","\tpixels = image_to_feature_vector(image)\n","\thist = extract_color_histogram(image)\n","\t# update the raw images, features, and labels matricies,\n","\t# respectively\n","\trawImages.append(pixels)\n","\tfeatures.append(hist)\n","\tlabels.append(label)\n","\t# show an update every 1,000 images\n","\tif i > 0 and i % 1000 == 0:\n","\t\tprint(\"[INFO] processed {}/{}\".format(i, len(imagePaths)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rJ8mrCi9RpC","colab_type":"code","colab":{}},"source":["# partition the data into training and testing splits, using 75%\n","# of the data for training and the remaining 25% for testing\n","(trainRI, testRI, trainRL, testRL) = train_test_split(\n","\trawImages, labels, test_size=0.25, random_state=42)\n","(trainFeat, testFeat, trainLabels, testLabels) = train_test_split(\n","\tfeatures, labels, test_size=0.25, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ntj4DObn9UOS","colab_type":"code","colab":{}},"source":["print(\"[INFO] evaluating raw pixel accuracy...\")\n","model = KNeighborsClassifier(n_neighbors=args[\"neighbors\"],\n","\tn_jobs=args[\"jobs\"])\n","model.fit(trainRI, trainRL)\n","acc = model.score(testRI, testRL)\n","print(\"[INFO] raw pixel accuracy: {:.2f}%\".format(acc * 100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iDQTOhSU9Vgg","colab_type":"code","colab":{}},"source":["# train and evaluate a k-NN classifer on the histogram\n","# representations\n","print(\"[INFO] evaluating histogram accuracy...\")\n","model = KNeighborsClassifier(n_neighbors=args[\"neighbors\"],\n","\tn_jobs=args[\"jobs\"])\n","model.fit(trainFeat, trainLabels)\n","acc = model.score(testFeat, testLabels)\n","print(\"[INFO] histogram accuracy: {:.2f}%\".format(acc * 100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MOQdg8jqVfbK","colab_type":"text"},"source":["# Generating Regional Proposal"]},{"cell_type":"code","metadata":{"id":"JEx4VYQGVkva","colab_type":"code","colab":{}},"source":["!pip install selectivesearch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NAAWFI4FVmRB","colab_type":"code","colab":{}},"source":["import selectivesearch\n","import skimage.io\n","import skimage\n","import matplotlib.patches as mpatches"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YzE3425FVnOG","colab_type":"code","colab":{}},"source":["def clip_pic(img, padding,x_min, y_min, x_max, y_max):\n","    x_min = x_min - padding\n","    y_min = y_min - padding\n","    x_min = max(0, x_min)\n","    y_min = max(0, y_min)\n","\n","    x_max = x_max + padding\n","    y_max = y_max + padding\n","    x_max = min(x_max, 224)\n","    y_max = min(y_max, 224)\n","    return img[y_min:y_max, x_min:x_max]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HwaQ7dICVqBU","colab_type":"code","colab":{}},"source":["def area_inter_val(xmin_a, xmax_a, ymin_a, ymax_a, xmin_b, xmax_b, ymin_b, ymax_b):\n","    \n","    intersect= not ( xmin_b > xmax_a or xmax_b < xmin_a or ymax_b < ymin_a or ymin_b > ymax_a)\n","\n","    if intersect == True:\n","        x_sorted_list = sorted([xmin_a, xmax_a, xmin_b, xmax_b])\n","        y_sorted_list = sorted([ymin_a, ymax_a, ymin_b, ymax_b])\n","        x_intersect_w = x_sorted_list[2] - x_sorted_list[1]\n","        y_intersect_h = y_sorted_list[2] - y_sorted_list[1]\n","        area_inter = x_intersect_w * y_intersect_h\n","        return area_inter\n","    else:\n","        return 0\n","\n","def iou_val(xmin_a, xmax_a, ymin_a, ymax_a, xmin_b, xmax_b, ymin_b, ymax_b):\n","    area_inter = area_inter_val(xmin_a, xmax_a, ymin_a, ymax_a, xmin_b, xmax_b, ymin_b, ymax_b)\n","    if area_inter > 0:\n","        area_1 = (xmax_a - xmin_a) * (ymax_a - ymin_a)\n","        area_2 = (xmax_b - xmin_b) * (ymax_b - ymin_b)\n","        return float(area_inter) / (area_1 + area_2 - area_inter)\n","    else:\n","        return 0\n","\n","def proposal_bbox(image_sample):\n","    _, regions = selectivesearch.selective_search(image_sample, scale=15, sigma=0.9, min_size=80)\n","    candidates = set()\n","    for r in regions:\n","        x, y, w, h = r['rect']\n","        if r['rect'] in candidates:\n","            continue\n","        if r['size'] < 50:\n","            continue\n","        if w == 0 or h == 0:\n","            continue\n","        candidates.add(r['rect'])\n","    return list(candidates)\n","\n","#[('NEUTROPHIL', (285, 169, 221, 206))]\n","\n","def regional_proposal(image_sample, proposal_bboxes):\n","    proposals = []\n","    for i in proposal_bboxes:\n","        x, y, w, h = i\n","        img_proposal = clip_pic(image_sample, 16, x, y, x+w, y+h)\n","        img_proposal2 = skimage.transform.resize(img_proposal, (224, 224))\n","        proposals.append((img_proposal2, (x,y,w,h)))\n","    return proposals"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tfXJNTXcVsqS","colab_type":"code","colab":{}},"source":["## save the image into folders\n","background_path = '/content/gdrive/My Drive/APS360/Train_Proposals/BACKGROUND/'\n","eo_path = '/content/gdrive/My Drive/APS360/Train_Proposals/EOSINOPHIL/'\n","lymph_path = '/content/gdrive/My Drive/APS360/Train_Proposals/LYMPHOCYTE/'\n","mono_path = '/content/gdrive/My Drive/APS360/Train_Proposals/MONOCYTE/'\n","neut_path = '/content/gdrive/My Drive/APS360/Train_Proposals/NEUTROPHIL/'\n","name = 'image_'\n","\n","\n","# generate proposal for 100 train images\n","all_proposals = []\n","for i in range(100):\n","    image_sample = train_imgs[i].copy()\n","    bbox = proposal_bbox(image_sample)\n","    proposals = regional_proposal(image_sample, bbox, train_labels[i])\n","    all_proposals.extend(proposals)\n","\n","cnt = 0\n","for j in range(len(all_proposals)):\n","    img_class = all_proposals[j][1]\n","    if img_class== 'BACKGROUND':\n","        cnt += 1\n","\n","        # select a portion from backgrounds\n","        if cnt % 20 == 0:\n","            matplotlib.image.imsave(background_path + name + str(j) + ext, all_proposals[j][0])\n","    elif img_class == 'NEUTROPHIL':\n","        matplotlib.image.imsave(neut_path + name + str(j) + ext, all_proposals[j][0])\n","    elif img_class == 'EOSINOPHIL':\n","        matplotlib.image.imsave(eo_path + name + str(j) + ext, all_proposals[j][0])\n","    elif img_class == 'LYMPHOCYTE':\n","        matplotlib.image.imsave(lymph_path + name + str(j) + ext, all_proposals[j][0])\n","    elif img_class == 'MONOCYTE':\n","        matplotlib.image.imsave(mono_path + name + str(j) + ext, all_proposals[j][0])\n","    else:\n","        print('err')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LAy-dmzTV9Vf","colab_type":"text"},"source":["# Transfer Learning and Training"]},{"cell_type":"code","metadata":{"id":"1Z1N4Bp2V6-U","colab_type":"code","colab":{}},"source":["import os\n","import time\n","import torchvision.models\n","\n","# load pretrained alexnet\n","model = torchvision.models.alexnet(pretrained=True).cuda()\n","\n","classes = ['EOSINOPHIL', 'MONOCYTE', 'NEUTROPHIL', 'LYMPHOCYTE', 'BACKGROUND' ]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AlBTF1nVWB7U","colab_type":"code","colab":{}},"source":["model.classifier[6] = nn.Linear(4096, 5).cuda()\n","nn.init.xavier_normal_(model.classifier[6].weight)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hXjMuf68WEgA","colab_type":"code","colab":{}},"source":["transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n","\n","t_dataset = torchvision.datasets.ImageFolder('/content/gdrive/My Drive/APS360/train_proposal-20200405T193852Z-001.zip (Unzipped Files)/train_proposal/', transform=transform)\n","evens = list(range(0, len(t_dataset), 2))\n","train_dataset = torch.utils.data.Subset(t_dataset, evens)\n","print(len(train_dataset))\n","\n","v_dataset = torchvision.datasets.ImageFolder('/content/gdrive/My Drive/APS360/val_proposal-20200405T195313Z-001.zip (Unzipped Files)/val_proposal/', transform=transform)\n","evens = list(range(0, len(v_dataset), 2))\n","val_dataset = torch.utils.data.Subset(v_dataset, evens)\n","print(len(val_dataset))\n","\n","\n","batch_size = 512\n","num_workers = 1\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n","                                           num_workers=num_workers, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, \n","                                           num_workers=num_workers, shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMzZbkLWWGKa","colab_type":"code","colab":{}},"source":["def get_accuracy_loss(model, data_loader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    total_loss = 0.0\n","    i = 0\n","    criterion = nn.CrossEntropyLoss()    \n","    with torch.no_grad():\n","        batch = 0\n","        for imgs, labels in data_loader:\n","\n","            if use_cuda and torch.cuda.is_available():\n","                imgs = imgs.cuda()\n","                labels = labels.cuda()\n","\n","            output = model(imgs)\n","            loss = criterion(output, labels)\n","            total_loss += loss.item()\n","            #select index with maximum prediction score\n","            pred = output.max(1, keepdim=True)[1]\n","            correct += pred.eq(labels.view_as(pred)).sum().item()\n","            total += imgs.shape[0]\n","            i+=1\n","            batch+=1\n","    final_loss = float(total_loss) / (i + 1)\n","    return correct / total, final_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EvUvOiD8WHn0","colab_type":"code","colab":{}},"source":["use_cuda = True\n","\n","def train_model(model, train_loader, val_loader, num_epochs=1, learn_rate = 0.001):\n","\n","    torch.manual_seed(1000)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n","\n","    train_acc, val_acc, train_loss, val_loss = [], [], [], []\n","\n","    # training\n","    print (\"Training Started...\")\n","    n = 0 # the number of epochs\n","    for epoch in range(num_epochs):\n","        start = time.time()\n","        j = 0\n","        model.train()\n","        for imgs, labels in iter(train_loader):\n","            \n","            if use_cuda and torch.cuda.is_available():\n","              imgs = imgs.cuda()\n","              labels = labels.cuda()\n","            out = model(imgs)             # forward pass\n","            loss = criterion(out, labels) # compute the total loss\n","            loss.backward()               # backward pass (compute parameter updates)\n","            optimizer.step()              # make the updates for each parameter\n","            optimizer.zero_grad()         # a clean up step for PyTorch\n","            j +=1\n","            print(j, end=' ')\n","            \n","        n += 1\n","        # track accuracy\n","        t_acc, t_loss = get_accuracy_loss(model, train_loader)\n","        v_acc, v_loss = get_accuracy_loss(model, val_loader)\n","\n","        train_acc.append(t_acc)\n","        train_loss.append(t_loss)\n","        val_acc.append(v_acc)\n","        val_loss.append(v_loss)\n","\n","        print('')\n","        print(\"epoch:\", epoch, end = \" \")\n","        print(\"train loss:\", train_loss[-1], end = \" | \")\n","        print(\"train acc:\", train_acc[-1], end = \" | \")\n","        print(\"val loss:\", val_loss[-1], end = \" | \")\n","        print(\"val acc:\", val_acc[-1])\n","        print('')\n","        \n","        end = time.time()\n","        print('time took: ', end-start)\n","    \n","    n = len(train_acc)\n","    plt.figure(0)\n","    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n","    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","\n","    plt.figure(1)\n","    plt.plot(range(1,n+1), train_acc, label=\"Train\")\n","    plt.plot(range(1,n+1), val_acc, label=\"Validation\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.legend(loc='best')\n","            \n","    return train_acc, val_acc\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iqu9rP3kWJmW","colab_type":"code","colab":{}},"source":["train_model(model=model, train_loader=train_loader, val_loader=val_loader, num_epochs = 5, learn_rate = 0.0004)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RV03m_FnWKh8","colab_type":"code","colab":{}},"source":["torch.save(model.state_dict(), '/content/gdrive/My Drive/APS360/alexnet2.pt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3FZp3AusWNZh","colab_type":"code","colab":{}},"source":["state = torch.load('/content/gdrive/My Drive/APS360/alexnet.pt')\n","model.load_state_dict(state)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pjgtYEMtWRhx","colab_type":"text"},"source":["# Testing"]},{"cell_type":"code","metadata":{"id":"5QqQ2-_EWQ87","colab_type":"code","colab":{}},"source":["transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n","\n","test_dataset = torchvision.datasets.ImageFolder('/content/gdrive/My Drive/APS360/test_proposal-20200405T193546Z-001.zip (Unzipped Files)/test_proposal', transform=transform)\n","evens = list(range(0, len(test_dataset), 2))\n","\n","batch_size = 512\n","num_workers = 1\n","\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, \n","                                           num_workers=num_workers, shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ySSUbuaaWVtX","colab_type":"code","colab":{}},"source":["acc, loss = get_accuracy_loss(model=model, data_loader=test_loader)\n","print('Test Accuracy: ', acc)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o3OipwTjWXyd","colab_type":"code","colab":{}},"source":["def get_wbc_counts(image):\n","\n","    # generate regional proposals\n","    image_sample = image.copy()\n","    proposals = regional_proposal(image_sample, bbox)\n","    for i in range(5):\n","        fig3, ax3 = plt.subplots(ncols=1, nrows=1, figsize=(4, 4))\n","        ax3.imshow(proposals[i][0])\n","\n","    classes = ['MONOCYTE','LYMPHOCYTE', 'EOSINOPHIL', 'NEUTROPHIL', 'BACKGROUND']\n","\n","    # get output for each proposal\n","    types = {}\n","    types['EOSINOPHIL'] = []\n","    types['LYMPHOCYTE'] = []\n","    types['MONOCYTE'] = []\n","    types['NEUTROPHIL'] = []\n","    for i in proposals:\n","        x = torch.tensor(i[0].reshape(-1, 3, 224, 224)).cuda()     # if numpy array\n","        x = x.float()\n","        out = model(x)\n","        out_class = out.max(1, keepdim=True)[1]\n","        if out_class != 'BACKGROUND':\n","            types[classes[out_class]].append(i[1]) # append its bbox to that category\n","\n","    # merge proposals if iou > 0.5, same label\n","    merged = {}\n","    for c in types.keys():\n","        bbox_list = types[c]\n","        merged_bboxes1 = merge_bbox(bbox_list)\n","        merged_bboxes = merge_by_iou(merged_bboxes1)\n","        merged[c] = merged_bboxes\n","\n","    return merged\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YhJOdOtmWwSc","colab_type":"code","colab":{}},"source":["test_sample = imread('/content/gdrive/My Drive/APS360/merged_378.jpg')\n","fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(3, 3))\n","ax.imshow(test_sample)\n","get_wbc_counts(test_sample)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-e5OXxIsT2KX","colab_type":"code","colab":{}},"source":["def confusion_matrix(model,data_loader):\n","    matrix = np.zeros(shape=(4,4))\n","\n","    n = 0\n","\n","    for imgs, labels in data_loader:\n","        \n","        if use_cuda and torch.cuda.is_available():\n","          imgs = imgs.cuda()\n","          labels = labels.cuda()\n","\n","        output = model(imgs)\n","        pred = output.max(1, keepdim=True)[1]\n","        for i in range(pred.shape[0]):\n","            matrix[labels[i]][pred[i]] +=1\n","        n+=1\n","        print(n)\n","\n","    return matrix"],"execution_count":0,"outputs":[]}]}